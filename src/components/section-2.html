<a id="section-2"></a>
<h2>Self-calibrating PIN-entering interface</h2>

<p>In this section we ask ourselves: What if the buttons had no colors? In concrete terms, what if the mapping between the position of the buttons (left/right) and their meaning (yellow/grey) was not pre-defined?</p>

<p>A typical user would be a bit confused about what to do and which button to press to enter their PIN. But let us assume the user decides to continue nonetheless and arbitrarily decides a color for each button and use the interface accordingly just as in section 1.</p>

<p>The machine is in trouble now, it does not know what digit the user wants to enter, and it does not know what the user means when pressing buttons. The reasoning used previously immediately collapses because we do not know the correspondance between the button's position and the button's color. Thus, in the example of section 1, we can no longer follow the logical path: "If the user presses the left button, then they mean that their digit is currently yellow".</p>

<p>If anything, this line of reasoning turns into: "If the user pressed the left button, then their digit is either yellow or grey, thus I cannot make any decision." End of the story? Of course not. But before explaining how we approach this problem, we think you should experience how it feels to use such an interface and be able to arbitrarily choose buttons' colors.</p>

<p>In <a href="#interaction-2">Interaction 2</a>, the interfaces works exactly the same way as in <a href="#interaction-1">Interaction 1</a> but this time colors are not displayed on the buttons, they are in your mind. You can use whatever color pattern you desire. Providing there is at least one button of each color and that you stick with the same pattern during the interaction, the machine will infer both your PIN and the color of the buttons. Try the interface multiple times, entering different digits and using different color patterns.</p>

<include src="src/figures/vault_3x3.html"></include>

<p>It is an interesting feeling, isn't it? We are not used to have this level of choice when using the machines around us.</p>

<p>Even when we are able to choose the way to use an interface, there usually is an explicit process for expressing that choice. We would first have to assign a color to each button using another interface (say a digital painting brush to color each button yellow or grey) and only then we could start using the interface following our coloring. In other words, there would be a dedicated and well defined interface to tell the machine the action-to-meaning mapping we want to use. But this is not the case here. The interface is self-calibrating, it understands both the digit and the colors at once, which differs from other adaptive human-computer interfaces. We will come back to this point later in section 5. For now, we ought to understand how we solve this problem.</p>

<p>In section 1, we defined the following components: intent, meaning and action. We understood that an action conveys a meaning that can be used to infer an intent. And we have seen that this logical path requires a context that allows to deduce meanings from actions and intents from meanings. In this section, we are breaking one of these links, we do not known the mapping between the user's actions and their meanings. Using our terminology, we can express the challenge as follows: Can we identify user intent if the mappipng between the user actions and their meaning is unknown?</p>

<p>To start answering this question, we must come back to our list of assumptions about the interaction. We know that the user is trying to type 1 out of 10 possible digits. We know that the principle of interacting is for the user to provide feedback about the color of the particular digit they have in mind. We know that a user press buttons to send this feedback and that one button is used to express only one color. The challenge becomes: Could we recombine these assumptions to identify the user intent nonetheless?</p>

<p>Another way to summarize the assumptions from above is that: "Users are assumed to be consistent with themselves and in their interaction with the machine, both according to their intent and the context in which the interaction takes place". Even if we do not know the colors of the buttons, the overarching assumption that the user is consistent holds<d-footnote>Removing the assumption of consistency would be throwing away all the principles of a user interface and would make any decision impossible, knowing the color of the buttons or not. Even in cases when we allow for user mistakes, we still assume, on average and in the long run, that the user is overall consistent. In such cases, we usually explicitly model user mistakes as noise in their actions selection process.</d-footnote>.</p>

<p>This notion of consistency will follow us all along this article. It is key to solving the self-calibrating problem because it is the only assumption we can safely rely on. It is an assumption that is implicit in all human-machine interaction scenarios. In this work, we not only make this assumption explicit but we make it a direct measure of the success of an interaction. But how can we measure consistency in our PIN entering interface?</p>

<p>Consistency is defined from two perspectives: (1) consistent with respect to an intent (clicking on a button of the same color as the intended digit) and (2) consistent with oneself (the same button is always used to mean the same color).</p>

<p>Consistency can be defined as wether the user's actions are aligned to the theoretical model of the user. For example, in section 1, a user is consistent if: "if their digit is grey, they press the grey button". If they press a yellow, they are not being consistent with respect to that digit. For a given digit, this translate in: "knowing that the user wants to type a '1', and that '1' is grey, then if a user presses a grey button, they are consistent, if they press a yellow button, they are inconsistent".</p>

<p>Notice how the reasoning is a bit different than in section 1. We start by saying: "knowing that the users wants to type a '1'". We make an hypothesis about the intent, and then check if the user's actions align with the expected actions from our model of the user. If it matches, we consider '1' as a plausible digit, if not, it is not plausible and we can discard it.</p>

<p>Following this definition of consistency and we can rewrite the logical of section 1 with two buttons of known colors as follows: "If the user wants to type a '1' and my model of a user is that they are using the left button to mean yellow and the right button to mean grey, then when they pressed the left button, they meant yellow which was inconsistent with the model I have of the user because the digit '1' was grey and I expected them to press the right button to mean grey, thus the user is not typing a '1'."</p>

<p>It is significantly more convoluted that the straitforward logics we used then, but it is exactly as valid and has the advantage of exposing the notion of consistency. Seeing the problem with this new pair of eyes is fundamental to understand the remaining of this article.</p>

<p>Having introduced consistency, we have not yet tackled the self-calibration problem. In the description above, the model of the user is already known. We expect the user to use the left button to mean yellow and the right button to mean grey. But when buttons have no colors, this part of our user model is not known.</p>

<p>If we do not have a user model, we have two choices: (1) we can make hypotheses about all possible user models and test them against observed user's actions, or (2) we can build models on the fly from the observations we make of the user's actions.</p>

<p>Option (1) would quickly run into combinatioral issues as, in additional to considering all possible user's intents, we should also consider all possible button-to-color mappings. It would remain manageable with the 9 buttons used in <a href="#interaction-2">Interaction 2</a> with <d-math>2^9=512</d-math> possible user models. But it would become impossible to handle in section 3 when we move to continuous actions. Actions being never twice the same, the combinatoriality of user models will grow with the number of interaction (<d-math>N</d-math>) in <d-math>2^N</d-math>, and more importantly consistency will be impossible to assess due to a lack of constraints. We will come back to this notion of constraints.</p>

<p>Option (2) sounds more interesting. Because we can observe the user's actions, we should be able to build a model of the user while the interaction unfolds. One problem remain: how can we build a model of a user if we do not know their intent, that is if we do not know what their actions mean? Constraints from the context tell us that the user's intent is to type 1 out of 10 possible digits. Thus, by assuming the user behaves consistently according to its intent, we can hypothesise and build as many models of the user as the set of possible intents. Compared to option (1), we do not have to build all possible models, but just as many as hypothesised intents, in our case only 10.</p>

<p>To sum up, we are bulding as many models of the user as possible intents. Each model predicting what the user might do next according to intent the  model was built from. From this model, we can evaluate the consistency of future user's actions. If a new observation deviate from the expectation from a model, then either the user is being inconsistent with his preivous self which can only indicate that the intent hypothesis used to build the model was wrong because our assumption is that the user is consistent.</p>

<p>Pluging this back to our logical statement: "If the user wants to type a '1', we can revisit the past history of its actions and build a model of the user as if they wanted to type a '1'. Then, when the user presses the next button, we can check wether their action is consistent with the model we just built with respect to the current color of the digit '1'. If yes, the user could be trying to type a 1, if not the user is certainly not trying to type a 1." </p>

<p>In very concrete terms, when, according to a given hypothesis, the same button seems to be used to mean both yellow and grey, the hypothesis can be discarded because it is imcompatible with our assumption of user consistency.</p>

<p>You can visualize this process directly on the <a href="#explanation-2">explanatory interface</a> below that displays a dedicated side panel showing the inner workings of the machine. As before, a tutorial video is available.</p>

<include src="src/figures/hood_3x3.html"></include>

<p>Interrestingly, once the machine identified a digit, the colors of the buttons that were pressed are also identified. It allows to freeze some part of the user model by coloring the corresponding buttons with their associated colors, and the next digit can be identified faster if those buttons are reused.</p>

<p>The remaining of this article expands on this idea of consistency but consider how to scale it to continuous user's actions. Buttons' presses are discrete and easily idenfiable actions which makes it easy to build a model and measure user's consistency. If you use the same button to mean both yellow and grey you are inconsistent. But when the user's actions are drawings, sounds, brain signals, or nerve impulses, they will never be represented twice in exactly the same way. This makes the problem more challenging as boolean logic will not be sufficient anymore. Lucklily, assumptions of consistency are paramount in machine learning algorithms and are directly expressed in their cost functions. We will exploit this to our advantage.</p>

<p>In the next section, you will discover a version of our interface with no buttons. Instead you will place points on a 2D map and you will get to decide which areas are associated to which color.</p>
