<a id="section-3"></a>
<h2>Can this approach scale to continuous signals?</h2>

<p>Up to now, we considered discrete button presses and our logic was based on identifying if the user was using the <b>same</b> button (action) for different colors (meaning). This notion of <b>same</b> was easily measurable with discrete button events. But when the user's actions are more complex, such as drawings, sounds, gestures, brain signals, or nerve impulses, the same action will never be represented twice in exactly the same way, they are continuous signals. It becomes impossible to define a notion of same preemptively, and our boolean reasoning can no longer prevail.</p>

<p>To explain this problem, we designed an interface with no button. Instead of pressing buttons, you have to place points on a map. The points can be placed in a yellow area to mean "My digit is yellow", or in a grey area to mean "My digit is grey". But the color map is not defined in advance. You decide in your mind which areas of the maps are yellow or grey and the machine has to figure it out along with your PIN.</p>

<p>You can try this below on <a href="#interaction-3">Interaction 3</a>. Start simple, for example typing the code 1234 by assuming the left part of the map is yellow and the right part is grey. Be patient, this is a hard problem, it might take 10 to 15 clicks to find the first digit. Once you are comfortable with this new interaction method, feel free to challenge the machine with more complex mappings. We recommend watching the associated video if unsure about what to do.</p>

<include src="src/figures/vault_touch.html"></include>

<p>Points you placed on the map are an example of continuous signals. You never clicked twice exactly in the same place. This means we cannot tell if two points represent the same color just by looking at them, even more so in the beginning when no structure has emerged from the data. Ask a friend to guess what you are doing and they will be clueless. So how are we to solve that problem?</p>

<p>To ground our explanation, we first need to squeeze the concepts covered so far into one word: <b>consistency</b>. This will help our brains to navigate this chapter. All we have done so far is measuring the consistency of the user when using our interface, we have been detecting breaches of consistency:</p>

<ul>
    <li>In <a href="#section-1">section 1</a>, we defined consistency as: clicking on a button of the same color as the digit we want to type. And a breach of consistency was looking for digits that were not of the same color as the button clicked by the user.</li>
    <li>In <a href="#section-2">section 2</a>, we defined consistency as: using a button to only mean one color - the "yellow or grey" assumption. And a breach of consistency was looking for the digits which, if a user was entering them, that user would have been pressing the same button to mean both yellow and grey.</li>
</ul>

<p>To scale our approach to continuous signals, we simply need a way to define and measure consistency for continuous signals. Previously we defined consistency with statements like "a button of the <b>same</b> color" and "only mean one color - yellow <b>or</b> grey". But the notion of "same" and "or" are no more applicable as all signals are different now. We need a more looser measure of similarity between signals.</p>

<p>While we cannot be in the mind of every person using this interface, we can nonetheless come up with broad principles of how most people should behave when deciding how to allocate colors and place points on the 2D map. For example, we can assume that users will define yellow and grey areas that are easy to differentiate - to be able to remember where to place a yellow or a grey point when required. Another common assumption is that the user will place points of the same color "close" to each others, where closeness is measured by the euclidean distance between them.</p>

<p>Summarizing these assumptions, we can defining consistency for continuous signals as: using a simple color map. Where simple is defined by the ability to easily differentiate between the yellow and grey points.</p>

<p>How can we measure that?</p>

<p>Machine learning experts invented <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. Given a set of colored points (the training set), a classifier can extrapolate and generate a color map that "best explains" the training set. Obviously, a number of assumptions must be made by machine learning experts to define this "best explains" criteria. These assumptions are baked into the cost functions that the classifiers are optimizing during their learning procedure.</p>

<p>A common assumption is that the simpler the map the better<d-footnote>See <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam's razor</a> principle and the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">biasâ€“variance tradeoff</a> in machine learning.</d-footnote>. This assumption is usually included as a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> term in the cost function to penalize solutions that leads to complicated maps. Complicated maps are usually defined as non-smooth maps with many sharp changes in their frontiers or that form a lot of isolated island.</p>

<p>How elegant. The assumptions baked into classifiers are the same as what we defined as consistency for our problem with continuous signals. Thinking twice, it is not that strange. These assumptions are made by humans trying to make sense of the world, they are meant to reflect the way we generate and classify things in the world.</p>

<p>How can we leverage classifiers to solve the self-calibration problem?</p>

<p>This is both easy and hard.</p>

<p>It is easy because classifiers provides metrics of consistency. Some good measures include the cross-validation classification accuracy or the value of the cost function after training. Other metrics can be imagined within the broad spectrum of machine learning tools.</p>

<p>The hard part is to decide when to stop. Because this is not a binary process, we can never be 100% sure that we have the right answer. But, because we know the user is entering 1 of 10 digits, we can run a comparative evaluation. As before, we can generate 10 different datasets, each with the same data points but different labels/colors according to each hypothesized digit. BY training a classifier on each dataset, we can measure their consistency using the metrics of our choice. Finally, we can run an adequate statistical test and define a threshold for which we are happy to claim that one hypothesis is statistically more consistent than all the others.<d-footnote>To be of real practical use, the very hard part is to decide when to stop in a way that is not too disruptive with the user experience. For that, the machine would have to be most of the time correct without exhausting the patience of the user. Sometimes it is be better to be wrong fast than to never be right because the user would otherwise gave up. This is harder to put into words, let alone in equation, and could be the subject to plenty of interesting research.</d-footnote></p> 

<p>This is how we solve the self-calibration problem in <a href="#interaction-3">Interaction 3</a>. We link to previous work detailing implementation details in section 5. For now, the all process is easier to understand visually from the side panel of explanatory interface 3 below. Try it and notice how each hypothesis assigns different labels to your actions. As a result, each hypothesis builds a different color map to explain your actions. After enough clicks, it becomes obvious which digit you are typing because all others hypotheses lead to more complex maps - indicating a breach in our definition of consistency for continuous signals: using a simple color map.</p>

<include src="src/figures/hood_touch.html"></include>

<p>I personally find this demo the most compelling in this article. Trying to challenge it with complex color maps, or trying to force false prediction, is a good exercise to verify you understand what is happening.</p>

<p>There are two important and counter-intuitive results that machine-learning connoisseurs should understand at this point:</p>

<ul>
    <li>
        
    <p><b>We do not use unsupervised clustering algorithms.</b> Instead we use constraints from the PIN entering task to generate a set of possible labels. We use these labels to train classifiers following supervised learning best practices. We make our decision by comparing the consistency of these classifiers, never using clustering analysis.</p>
        
    <p>This gives a significant advantage compared to unsupervised clustering analysis because we only have to consider a limited set of possible labelling. In our PIN entering interface, we only have to choose between 10 possible labelling.</p> 
    
    <p>This leads to some surprising results as illustrated in the examples in the video and color mapping below. We used the interface by pacing point within two clearly defined clusters one on the right side and one on the left side. But the color map splits the screen along the vertical axis, top for yellow and bottom for grey.</p>
    
    <p>With this configuration any method based on usual clustering assumptions is bound to fail, while our method can still find the correct mapping. This is because a clustering approach assumes the user's input can be split into well identifiable clusters. Our method is instead looking for a consistent mapping - the simplest color map that explains the data within the constraints given by the task.</p>

    <p>To understand better why this works, we encourage you to challenge the interface with confusing data where the data structure does not match directly with the color mapping.</p>

    </li>

    <li>
        
    <p><b>We never freeze the classifier.</b> Once a first digit is identified, the classifier/map can still be changed significantly in areas that have no point yet.</p>
        
    <p>This differs from how interface are usually designed. Typically, a calibration step is performed first, where labeled data are collected using a known protocol and a classifier is trained on these data. Then, the classifier is "frozen" and used to translate signals from users into their meanings. Two problems arises when users generate signals far out from the range of data in the training set. First the classifier nonetheless makes a prediction which might be erroneous leading to undesired results. Second, the classifier will not be updated with this new data point and the problem will reoccur in the future.</p>

    <p>In our work, we never use the predictions from a particular classifier to make decisions. We always and only compare the consistency of hypothetical classifiers and decide which one is significantly more likely to explain the user behavior.</p>
        
    <p>Once a decision is made, we do not freeze the associated classifier to re-use for the next digits. Rather, we propagate the labels associated to the "winning" hypothesis to all other hypothesis. However, and of key importance, all subsequent user's actions will continue be assigned different labels for each hypothesis according to their respective constraints. This process will drive away the hypothesis classifiers again and a new decision will be made when one of the classifiers is significantly more likely to explain the user behavior. </p> 

    <p>This leads to some surprising results, which is best visualized below. We defined three areas on the map - Left/Middle/Right. For the first digit, we use the left area for yellow, and the middle for grey, but do not use the right area. Once the first digit is found, we enter the second digit by continuing using the middle area for grey, but by now using the right area for yellow, and never using again the left area. If we were freezing and using the best classifier learned for the first digit, all clicks in the right area would be predicted to be grey. But with our method, the machine is not lead ashtray, does not over generalize and can identify the correct digit and learn that the right area is used to mean yellow.</p>

    

    <p>Interestingly, as more digits are identified, the decision process becomes faster thanks to the process of propagation of known labels after each identification. A larger and larger proportion of signal-label pairs are shared between all hypothesis, making it easier to detect inconsistencies. One grey point landing in the middle of a pool of 20 yellow dots shared by each hypothesis becomes a strong sign of a breach in consistency.</p>
        
    <p>This is the continuous equivalent of the process described in section 2 with discrete buttons. In section 2, once a first digit is identified, the color of the buttons pressed so far becomes known. Pressing these buttons again indicates directly which color the user refers to as all hypothesis share the same understanding of these buttons. However, pressing a black button does not provide such information and the breach in consistency procedure needs to be employed. Finally, once the second digit is identified, the color of the newly pressed button is know, and the process repeats for the third and fourth digit.</p>
        
    <p>With continuous signals, such certainties does not exist but can be approximated. Once a first digit is identified, the color of the points observed so far becomes known. Placing a new point very close to the previous location makes it more likely to be of the same color. Due to label propagation, all hypothesis now share the same colors for previous points, and the classifiers are thus more likely to be similar in these areas. However, placing a point in a location that was never observed before does not provide as much information because each classifier might be able to integrate this new point in their own way without conflicting with other data. Thus we need more evidence to detect breaches in consistencies in these areas. Finally, once the second digit is found, we can propagate the labels and all hypothesis will agree on a revised color map. Only for it to diverge again as the process repeats for the third and fourth digits.</p>

    </li>

</ul>

<p>LINK TO ALL DEMO VIDEOS showing the crazy thing we can do</p>

<p>The pandora box is now open. If we can solve the self-calibration problem with continuous user signals, we can theoretically apply it to wide range of modalities, such as speech.</p>

<p>Indeed, machine learning experts spent tremendous efforts to represent their favorite phenomenon into continuous vectors, with the aim to train and build models. Speech, drawings, images, gestures, brain signals, you name it, the scientific community found ways to represent them in vector form. It should thus be possible to use our interface with any such signals.</p> 

<p>With no surprise, in the next section, you will use drawings and spoken commands to enter a pin in our interface and following the self-calibrating approach.</p>
