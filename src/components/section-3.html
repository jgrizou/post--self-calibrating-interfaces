<a id="section-3"></a>
<h2>Can this approach scale to continuous signals?</h2>

<p>Up to now, we considered discrete button presses and our logic was based on identifying if the user was using the <b>same</b> button (action) for different colors (meaning). This notion of <b>same</b> was easily measurable with discrete button events. But when the user's actions are more complex, such as drawings, sounds, gestures, brain signals, or nerve impulses, the same action will never be represented twice in exactly the same way, they are continuous signals. It becomes impossible to define a notion of same preemptively, and our boolean reasoning can no longer prevail.</p>

<p>To explain this problem, we designed an interface with no button. Instead of pressing buttons, you have to place points on a map. The points can be placed in a yellow area to mean "My digit is yellow", or in a grey area to mean "My digit is grey". But the color map is not defined in advance. You decide in your mind which areas of the maps are yellow or grey and the machine have to figure it out along with your PIN.</p>

<p>You can try this below on <a href="#interaction-3">Interaction 3</a>. Start simple, for example typing the code 1234 by assuming the left part of the map is yellow and the right part is grey. Be patient, this is a hard problem, it might take 10 to 15 clicks to find the first digit. Once you are comfortable with this new interaction method, feel free to challenge the machine with more complex mappings. We recommend watching the associated video if unsure about what to do.</p>

<include src="src/figures/vault_touch.html"></include>

<p>Points on the map are an example of continuous signals. You never clicked twice exactly on the same place. This means we cannot tell if two points represent the same color just by looking at them, even more so in the beginning when no structure has emerged from the data. Ask a friend to guess what you are doing and they will be clueless. So how are we to solve that problem?</p>

<p>To ground our explanation, we first need to squeeze the concepts covered so far into one word: consistency. This will help our brains to navigate this chapter. All we have done so far is measuring the consistency of the user when using our interface, and more precisely detecting breaches of consistency:</p>

<ul>
    <li>In <a href="#section-1">section 1</a>, we defined consistency as: clicking on a button of the same color as the digit we want to type. And a breach of consistency was looking for digits that were not of the same color as the button clicked by the user.</li>
    <li>In <a href="#section-2">section 2</a>, we defined consistency as: using a button to only mean one color - the "yellow or grey" assumption. And a breach of consistency was looking for the digits which, if the user was entering them, they would have been pressing the same button to mean both yellow and grey.</li>
</ul>

<p>To scale our approach to continuous signals, we simply need a way to define and measure consistency for continuous signals. Previously we defined consistency with statements like "a button of the <b>same</b> color" and "only mean one color - yellow <b>or</b> grey". But the notion of "same" and "or" are no more applicable, we need a more looser measure of similarity between signals.</p>

<p>While we cannot be in the mind of every person using this interface, we can nonetheless come up with broad principles of how most people should behave when deciding how to allocate colors and place points on the 2D map. For example, we can assume that users will define yellow and grey areas that are easy to differentiate - to be able to remember where to place a yellow or a grey point when required. Another common assumption is that the user will place points of the same color "close" to each others, where closeness is measured by the euclidean distance between them.</p>

<p>We can summarize these assumptions by defining consistency for continuous signals as: using a simple action-to-meaning mapping. Where simple is defined by the ability to easily  differentiate between the yellow and grey points.</p>

<p>How can we measure that?</p>

<p>Machine learning experts invented <a href="https://en.wikipedia.org/wiki/Statistical_classification">classifiers</a>. Given a set of colored points (the training set), a classifier can extrapolate and generate a color map that "best explains" the training set. Obviously, a number of assumptions must be made by machine learning experts to define this "best explains" criteria. These assumptions are baked into the cost functions that the classifiers are optimizing during their learning procedure.</p>

<p>A common assumption is that the simpler the map the better<d-footnote>See <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam's razor</a> principle and the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">biasâ€“variance tradeoff</a> in machine learning.</d-footnote>. This assumption is usually included as a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> term in the cost function to penalize solutions that leads to complicated maps. Complicated maps are maps that are not smooth with many sharp changes on their frontiers or that form a lot of isolated island.</p>

<p>How elegant. The assumptions baked into classifiers are the same as what we defined as consistency for our problem with continuous signals. Thinking twice, it is not that strange. These assumptions are made by humans trying to make sense of the world, they are meant to reflect the way we generate and classify things in the world.</p>

<p>How can we leverage classifiers to solve the self-calibration problem?</p>

<p>This is both easy and hard.</p>

<p>It is easy because ass explained above classifiers provides us with a direct measure of consistency. Good measures include the cross-validation classification accuracy of a dataset, or the value of the cost function after training. Other metrics can be imagined as easily within the broad spectrum of machine learning tools.</p>

<p>The hard part is to decide when to stop. Because this is not a binary process, we can never be 100% sure of the right answer. But we can run a comparative evaluation. Because we know the user is entering 1 of 10 digits, we have 10 hypotheses. As before, we can generate 10 different datasets, each with the same data points but different labels/colors. We then measure the consistency of each datasets using the metrics of our choice. And finally run our preferred statistical test and define a threshold for which we can claim that one hypothesis is statistically more consistent than all the others.<d-footnote>To be of real practical use, the very hard part is to decide when to stop in a way that is not too disruptive with the user experience. For that, the machine would have to be most the time correct without exhausting the patience of the user. Sometimes it is be better to be wrong fast than to never be right because the user gave up. This is harder to put into words, let alone in equation, and could be the subject to plenty of interesting research.</d-footnote></p> 

<p>This is how we solve self-calibration problem in the 2D map interface you tried above. We link to previous work explaining the implementation details in section 5. For now, the all process is very easy to understand visually from the side panel of explanatory interface 3 below. Try it and notice how each hypothesis assign different labels to your actions. As a result, each hypothesis builds a different color map to explain your actions. After enough clicks, it becomes obvious which digit you are typing because all others hypotheses lead to more complex maps - indicating a breach in our definition of consistency for continuous signals: using a simple action-to-meaning mapping.</p>

<include src="src/figures/hood_touch.html"></include>

<p>I personally find this demo the most compelling in this article. Trying to challenge it with complex color maps, or trying to force false prediction, is a good exercise to verify you understand what is happening.</p>

<p>There is two very important and counter-intuitive facts that machine-learning connoisseurs should understand from this demo:</p>

<ul>
    <li>We do not use any sort of unsupervised clustering algorithms. This is best demonstrated by the demo in this video and in the resulting image below. Using the interface with two clearly defined clusters one on the right and one o the left. But the color split is actually top and bottom. Any attempts to use of unsupervised clustering approach would fail with these data.</li>
    <p>Before moving to the next section, we should talk about unsupervised learning. Like we discussed in section 2 about the possibility to geenrate coupled hypothesis on the intent+action to meaning, the same idea could be brought back here by first using a clusterign algrotihm to find clusters and then consider all combinations of class for these clusters. The combinatoriality could rise really quickly indeed. But more importantly it would be a very limited approach in the real word as clustering require a lot more data that our apporahc requires, and it simply does not work well when the data are very close or overlapping. Clusteting is an extreme case of using consistency as an assumption as it requires relaitvely well separated data. The approach presented here is more powerful because we actually use labels, and labe can inform us ont he strcuture of the data in way unsupervide clustering cannot. We will come back to this point in section 5.</p>

    <li>Once the first digit is identified, the map can still be changed significantly. This differs from usual work where first a calibration phase is done to collect labeled data and train a classifier. And then the classifier is used to interpret the signals. Here we never use directly the precision from a particular classifier to make our decision, but we compare the quality of 10 different classifiers to decide which on is the most likely to be explain the user. This is best described by typing by defining three areas on the map. Left/Middle/Right. For the first digit, we use the left area for yellow, and the middle for grey. We do not use the right area. Once the first digit is found, we type the second digit by continuing using the middle area for grey, but by now using the right area for yellow, and never using again the left area. If we were freezing and using the best classifier learned during the learning of the first digit, all click in the right area would be predicted to be grey. But with our method, the machine is not disturbed, does not over generalize and can identify the correct digit and learn that the right area is actually used to mean yellow.</li>
    <p>A important thing to note is how we realocated all the labels once we have identified the first digit. When we are sure about the digit, we are sure of the labels, thus we can propagate them to all hypothesis, they are a common truth. But all subsequent user's actions will have again different interpreatation according to each hypothesis, driving away the classifiers again, but making it faster for the machine to identify the solution as one grey point in the middle of a yellow pool of dot, will be a strog sign of insocnsitencies.</p>

</ul>

<p>LINK TO ALL DEMO VIDEOS showing the crazy thing we can do</p>

<p>Now that we could solve the self-calibration problem with continuous user signals, the pandora box is fully openned. A big game in machien learning is to represetnt he data int he form of vectors that can be used by these algrothms. So a lot of work has been done to represent speech, drawings, gestures, brain signals, and about all you can think about.</p>

<p>In the next section, we demonstrate the use of drawing and speech to enter a pin in our interface, entering firmly in the space of language learning and providing concrete examples of what self-calibrating devices can do.</p>
